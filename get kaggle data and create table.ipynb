{"cells":[{"cell_type":"code","source":["# Downloading dataset from kaggle\n!pip install kaggle\n!mkdir ~/.kaggle\n!touch ~/.kaggle/kaggle.json\n\nimport json\napi_token = {\"username\":\"shyamcb97\",\"key\":\"84a1284bd84faf1381d9a0d68ea8990f\"}\nwith open('/root/.kaggle/kaggle.json', 'w') as file:\n    json.dump(api_token, file)\n\n!chmod 600 ~/.kaggle/kaggle.json\n\nimport kaggle\n!kaggle datasets download -d berkayalan/retail-sales-data\n!unzip -d ./retail-sales-data ./retail-sales-data.zip\n\n\n\ndf.write.format(\"parquet\").saveAsTable(\"retail_data_turkish\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5c312fba-8e4d-4c0a-8731-5b13e1cf4528"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Collecting kaggle\r\n  Downloading kaggle-1.5.12.tar.gz (58 kB)\r\n\u001B[?25l\r\u001B[K     |█████▋                          | 10 kB 21.2 MB/s eta 0:00:01\r\u001B[K     |███████████▏                    | 20 kB 8.4 MB/s eta 0:00:01\r\u001B[K     |████████████████▊               | 30 kB 5.3 MB/s eta 0:00:01\r\u001B[K     |██████████████████████▎         | 40 kB 5.5 MB/s eta 0:00:01\r\u001B[K     |███████████████████████████▉    | 51 kB 5.7 MB/s eta 0:00:01\r\u001B[K     |████████████████████████████████| 58 kB 3.9 MB/s \r\n\u001B[?25hRequirement already satisfied: six>=1.10 in /databricks/python3/lib/python3.8/site-packages (from kaggle) (1.15.0)\r\nRequirement already satisfied: certifi in /databricks/python3/lib/python3.8/site-packages (from kaggle) (2020.12.5)\r\nRequirement already satisfied: python-dateutil in /databricks/python3/lib/python3.8/site-packages (from kaggle) (2.8.1)\r\nRequirement already satisfied: requests in /databricks/python3/lib/python3.8/site-packages (from kaggle) (2.25.1)\r\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.8/site-packages (from kaggle) (4.59.0)\r\nCollecting python-slugify\r\n  Downloading python_slugify-6.1.1-py2.py3-none-any.whl (9.1 kB)\r\nRequirement already satisfied: urllib3 in /databricks/python3/lib/python3.8/site-packages (from kaggle) (1.25.11)\r\nCollecting text-unidecode>=1.3\r\n  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\r\n\u001B[?25l\r\u001B[K     |████▏                           | 10 kB 11.7 MB/s eta 0:00:01\r\u001B[K     |████████▍                       | 20 kB 5.6 MB/s eta 0:00:01\r\u001B[K     |████████████▋                   | 30 kB 7.8 MB/s eta 0:00:01\r\u001B[K     |████████████████▊               | 40 kB 5.9 MB/s eta 0:00:01\r\u001B[K     |█████████████████████           | 51 kB 4.1 MB/s eta 0:00:01\r\u001B[K     |█████████████████████████▏      | 61 kB 4.7 MB/s eta 0:00:01\r\u001B[K     |█████████████████████████████▍  | 71 kB 5.0 MB/s eta 0:00:01\r\u001B[K     |████████████████████████████████| 78 kB 2.5 MB/s \r\n\u001B[?25hRequirement already satisfied: chardet<5,>=3.0.2 in /databricks/python3/lib/python3.8/site-packages (from requests->kaggle) (4.0.0)\r\nRequirement already satisfied: idna<3,>=2.5 in /databricks/python3/lib/python3.8/site-packages (from requests->kaggle) (2.10)\r\nBuilding wheels for collected packages: kaggle\r\n  Building wheel for kaggle (setup.py) ... \u001B[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n\u001B[?25h  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73053 sha256=e2c6c64a2bb7007bcc67424844bda3ea89b5d6837015ac784b7c7c58e423c2c5\r\n  Stored in directory: /root/.cache/pip/wheels/29/da/11/144cc25aebdaeb4931b231e25fd34b394e6a5725cbb2f50106\r\nSuccessfully built kaggle\r\nInstalling collected packages: text-unidecode, python-slugify, kaggle\r\nSuccessfully installed kaggle-1.5.12 python-slugify-6.1.1 text-unidecode-1.3\r\n\u001B[33mWARNING: You are using pip version 21.0.1; however, version 22.0.4 is available.\r\nYou should consider upgrading via the '/databricks/python3/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\nDownloading retail-sales-data.zip to /databricks/driver\r\n\r  0%|                                               | 0.00/86.5M [00:00<?, ?B/s]\r  6%|██▏                                   | 5.00M/86.5M [00:00<00:01, 47.7MB/s]\r 16%|██████▏                               | 14.0M/86.5M [00:00<00:01, 68.2MB/s]\r 24%|█████████▏                            | 21.0M/86.5M [00:00<00:01, 37.8MB/s]\r 30%|███████████▍                          | 26.0M/86.5M [00:00<00:02, 27.1MB/s]\r 38%|██████████████▍                       | 33.0M/86.5M [00:01<00:01, 31.0MB/s]\r 47%|██████████████████                    | 41.0M/86.5M [00:01<00:01, 32.3MB/s]\r 54%|████████████████████▋                 | 47.0M/86.5M [00:01<00:01, 37.3MB/s]\r 60%|██████████████████████▊               | 52.0M/86.5M [00:01<00:00, 38.4MB/s]\r 66%|█████████████████████████             | 57.0M/86.5M [00:01<00:00, 36.8MB/s]\r 75%|████████████████████████████▌         | 65.0M/86.5M [00:01<00:00, 35.4MB/s]\r 84%|████████████████████████████████      | 73.0M/86.5M [00:02<00:00, 38.0MB/s]\r 92%|███████████████████████████████████▏  | 80.0M/86.5M [00:02<00:00, 44.3MB/s]\r 98%|█████████████████████████████████████▎| 85.0M/86.5M [00:02<00:00, 45.7MB/s]\r\n\r100%|██████████████████████████████████████| 86.5M/86.5M [00:02<00:00, 38.9MB/s]\r\nArchive:  ./retail-sales-data.zip\r\n  inflating: ./retail-sales-data/product_hierarchy.csv  \r\n  inflating: ./retail-sales-data/sales.csv/sales.csv  \r\n  inflating: ./retail-sales-data/store_cities.csv  \r\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Collecting kaggle\r\n  Downloading kaggle-1.5.12.tar.gz (58 kB)\r\n\u001B[?25l\r\u001B[K     |█████▋                          | 10 kB 21.2 MB/s eta 0:00:01\r\u001B[K     |███████████▏                    | 20 kB 8.4 MB/s eta 0:00:01\r\u001B[K     |████████████████▊               | 30 kB 5.3 MB/s eta 0:00:01\r\u001B[K     |██████████████████████▎         | 40 kB 5.5 MB/s eta 0:00:01\r\u001B[K     |███████████████████████████▉    | 51 kB 5.7 MB/s eta 0:00:01\r\u001B[K     |████████████████████████████████| 58 kB 3.9 MB/s \r\n\u001B[?25hRequirement already satisfied: six>=1.10 in /databricks/python3/lib/python3.8/site-packages (from kaggle) (1.15.0)\r\nRequirement already satisfied: certifi in /databricks/python3/lib/python3.8/site-packages (from kaggle) (2020.12.5)\r\nRequirement already satisfied: python-dateutil in /databricks/python3/lib/python3.8/site-packages (from kaggle) (2.8.1)\r\nRequirement already satisfied: requests in /databricks/python3/lib/python3.8/site-packages (from kaggle) (2.25.1)\r\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.8/site-packages (from kaggle) (4.59.0)\r\nCollecting python-slugify\r\n  Downloading python_slugify-6.1.1-py2.py3-none-any.whl (9.1 kB)\r\nRequirement already satisfied: urllib3 in /databricks/python3/lib/python3.8/site-packages (from kaggle) (1.25.11)\r\nCollecting text-unidecode>=1.3\r\n  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\r\n\u001B[?25l\r\u001B[K     |████▏                           | 10 kB 11.7 MB/s eta 0:00:01\r\u001B[K     |████████▍                       | 20 kB 5.6 MB/s eta 0:00:01\r\u001B[K     |████████████▋                   | 30 kB 7.8 MB/s eta 0:00:01\r\u001B[K     |████████████████▊               | 40 kB 5.9 MB/s eta 0:00:01\r\u001B[K     |█████████████████████           | 51 kB 4.1 MB/s eta 0:00:01\r\u001B[K     |█████████████████████████▏      | 61 kB 4.7 MB/s eta 0:00:01\r\u001B[K     |█████████████████████████████▍  | 71 kB 5.0 MB/s eta 0:00:01\r\u001B[K     |████████████████████████████████| 78 kB 2.5 MB/s \r\n\u001B[?25hRequirement already satisfied: chardet<5,>=3.0.2 in /databricks/python3/lib/python3.8/site-packages (from requests->kaggle) (4.0.0)\r\nRequirement already satisfied: idna<3,>=2.5 in /databricks/python3/lib/python3.8/site-packages (from requests->kaggle) (2.10)\r\nBuilding wheels for collected packages: kaggle\r\n  Building wheel for kaggle (setup.py) ... \u001B[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n\u001B[?25h  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73053 sha256=e2c6c64a2bb7007bcc67424844bda3ea89b5d6837015ac784b7c7c58e423c2c5\r\n  Stored in directory: /root/.cache/pip/wheels/29/da/11/144cc25aebdaeb4931b231e25fd34b394e6a5725cbb2f50106\r\nSuccessfully built kaggle\r\nInstalling collected packages: text-unidecode, python-slugify, kaggle\r\nSuccessfully installed kaggle-1.5.12 python-slugify-6.1.1 text-unidecode-1.3\r\n\u001B[33mWARNING: You are using pip version 21.0.1; however, version 22.0.4 is available.\r\nYou should consider upgrading via the '/databricks/python3/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\nDownloading retail-sales-data.zip to /databricks/driver\r\n\r  0%|                                               | 0.00/86.5M [00:00<?, ?B/s]\r  6%|██▏                                   | 5.00M/86.5M [00:00<00:01, 47.7MB/s]\r 16%|██████▏                               | 14.0M/86.5M [00:00<00:01, 68.2MB/s]\r 24%|█████████▏                            | 21.0M/86.5M [00:00<00:01, 37.8MB/s]\r 30%|███████████▍                          | 26.0M/86.5M [00:00<00:02, 27.1MB/s]\r 38%|██████████████▍                       | 33.0M/86.5M [00:01<00:01, 31.0MB/s]\r 47%|██████████████████                    | 41.0M/86.5M [00:01<00:01, 32.3MB/s]\r 54%|████████████████████▋                 | 47.0M/86.5M [00:01<00:01, 37.3MB/s]\r 60%|██████████████████████▊               | 52.0M/86.5M [00:01<00:00, 38.4MB/s]\r 66%|█████████████████████████             | 57.0M/86.5M [00:01<00:00, 36.8MB/s]\r 75%|████████████████████████████▌         | 65.0M/86.5M [00:01<00:00, 35.4MB/s]\r 84%|████████████████████████████████      | 73.0M/86.5M [00:02<00:00, 38.0MB/s]\r 92%|███████████████████████████████████▏  | 80.0M/86.5M [00:02<00:00, 44.3MB/s]\r 98%|█████████████████████████████████████▎| 85.0M/86.5M [00:02<00:00, 45.7MB/s]\r\n\r100%|██████████████████████████████████████| 86.5M/86.5M [00:02<00:00, 38.9MB/s]\r\nArchive:  ./retail-sales-data.zip\r\n  inflating: ./retail-sales-data/product_hierarchy.csv  \r\n  inflating: ./retail-sales-data/sales.csv/sales.csv  \r\n  inflating: ./retail-sales-data/store_cities.csv  \r\n"]}}],"execution_count":0},{"cell_type":"code","source":["import pandas as pd\ndf_p=pd.read_csv('./retail-sales-data/sales.csv/sales.csv')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2bad95a4-97b6-4081-b8d1-a3aacefa1d34"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"/databricks/python/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (10,12) have mixed types.Specify dtype option on import or set low_memory=False.\n  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["/databricks/python/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (10,12) have mixed types.Specify dtype option on import or set low_memory=False.\n  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"]}}],"execution_count":0},{"cell_type":"code","source":["df=spark.createDataFrame(df_p)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"781f958f-8909-4346-8697-0462ffbd5ef6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["temp_table_name = \"retail_data_turkish\"\n\ndf.createOrReplaceTempView(temp_table_name)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"71283ad7-18d2-47ac-95b3-1aa6be08ca19"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.write.format(\"parquet\").saveAsTable(\"retail_data_turkish\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"082ec605-c855-4ddb-9284-edb2f585f9a3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# File location and type\nfile_location = \"tmp/retail-sales-data/sales.csv/sales.csv\"\nfile_type = \"parquet\"\n\n# CSV options\ninfer_schema = \"false\"\nfirst_row_is_header = \"false\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cd51bafc-f8ae-418e-af79-c864ae7b12db"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\n\u001B[0;32m<command-2686013685623490>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;31m# The applied options are for CSV files. For other file types, these will be ignored.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m \u001B[0mdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfile_type\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m   \u001B[0;34m.\u001B[0m\u001B[0moption\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"inferSchema\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minfer_schema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m   \u001B[0;34m.\u001B[0m\u001B[0moption\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"header\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfirst_row_is_header\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(self, path, format, schema, **options)\u001B[0m\n\u001B[1;32m    202\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    203\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 204\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    205\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mpath\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    206\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    121\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 123\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    124\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    125\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mIllegalArgumentException\u001B[0m: Path must be absolute: tmp/retail-sales-data/sales.csv/sales.csv","errorSummary":"<span class='ansi-red-fg'>IllegalArgumentException</span>: Path must be absolute: tmp/retail-sales-data/sales.csv/sales.csv","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\n\u001B[0;32m<command-2686013685623490>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;31m# The applied options are for CSV files. For other file types, these will be ignored.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m \u001B[0mdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfile_type\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m   \u001B[0;34m.\u001B[0m\u001B[0moption\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"inferSchema\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minfer_schema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m   \u001B[0;34m.\u001B[0m\u001B[0moption\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"header\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfirst_row_is_header\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(self, path, format, schema, **options)\u001B[0m\n\u001B[1;32m    202\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    203\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 204\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    205\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mpath\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    206\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    121\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 123\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    124\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    125\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mIllegalArgumentException\u001B[0m: Path must be absolute: tmp/retail-sales-data/sales.csv/sales.csv"]}}],"execution_count":0},{"cell_type":"code","source":["ls /tmp/retail-sales-data/sales.csv/sales.csv\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6054fed3-8d92-433b-9c34-045e2ad010ff"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"/tmp/retail-sales-data/sales.csv/sales.csv\r\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["/tmp/retail-sales-data/sales.csv/sales.csv\r\n"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e8dcc2ed-cecf-4cdd-a13c-6fc12d8fff1d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\u001B[0m\u001B[01;32mBUILD\u001B[0m*      \u001B[01;34mboot\u001B[0m/        \u001B[01;34mdev\u001B[0m/   \u001B[01;36mlib\u001B[0m@    \u001B[01;36mlibx32\u001B[0m@       \u001B[01;34mmnt\u001B[0m/   \u001B[01;34mroot\u001B[0m/  \u001B[01;34msrv\u001B[0m/  \u001B[01;34musr\u001B[0m/\r\n\u001B[01;34mWorkspace\u001B[0m/  \u001B[01;34mdatabricks\u001B[0m/  \u001B[01;34metc\u001B[0m/   \u001B[01;36mlib32\u001B[0m@  \u001B[01;34mlocal_disk0\u001B[0m/  \u001B[01;34mopt\u001B[0m/   \u001B[01;34mrun\u001B[0m/   \u001B[01;34msys\u001B[0m/  \u001B[01;34mvar\u001B[0m/\r\n\u001B[01;36mbin\u001B[0m@        \u001B[01;34mdbfs\u001B[0m/        \u001B[01;34mhome\u001B[0m/  \u001B[01;36mlib64\u001B[0m@  \u001B[01;34mmedia\u001B[0m/        \u001B[01;34mproc\u001B[0m/  \u001B[01;36msbin\u001B[0m@  \u001B[30;42mtmp\u001B[0m/\r\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0m\u001B[01;32mBUILD\u001B[0m*      \u001B[01;34mboot\u001B[0m/        \u001B[01;34mdev\u001B[0m/   \u001B[01;36mlib\u001B[0m@    \u001B[01;36mlibx32\u001B[0m@       \u001B[01;34mmnt\u001B[0m/   \u001B[01;34mroot\u001B[0m/  \u001B[01;34msrv\u001B[0m/  \u001B[01;34musr\u001B[0m/\r\n\u001B[01;34mWorkspace\u001B[0m/  \u001B[01;34mdatabricks\u001B[0m/  \u001B[01;34metc\u001B[0m/   \u001B[01;36mlib32\u001B[0m@  \u001B[01;34mlocal_disk0\u001B[0m/  \u001B[01;34mopt\u001B[0m/   \u001B[01;34mrun\u001B[0m/   \u001B[01;34msys\u001B[0m/  \u001B[01;34mvar\u001B[0m/\r\n\u001B[01;36mbin\u001B[0m@        \u001B[01;34mdbfs\u001B[0m/        \u001B[01;34mhome\u001B[0m/  \u001B[01;36mlib64\u001B[0m@  \u001B[01;34mmedia\u001B[0m/        \u001B[01;34mproc\u001B[0m/  \u001B[01;36msbin\u001B[0m@  \u001B[30;42mtmp\u001B[0m/\r\n"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a555a23-6061-4b29-b1b2-1d7b1c336f17"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Retail data EDA","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2447857353342461}},"nbformat":4,"nbformat_minor":0}
